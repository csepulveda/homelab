operator:
  crds:
    enabled: true
  
  monitoring:
    enabled: false
  
  # Deshabilitamos el operador CSI que causa problemas con CRDs grandes
  csi:
    rookUseCsiOperator: false
    enableRbdDriver: false
    enableCephfsDriver: false
    enableNFSDriver: false
    enableCephfsSnapshotter: false
    enableRBDSnapshotter: false
    enableVolumeReplication: false
  
  enableDiscoveryDaemon: false  # Deshabilitado para evitar detección automática de discos
  
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

cluster:
  operatorNamespace: rook-ceph
  
  # Configuración del Ingress
  ingress:
    dashboard:
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt
        nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
        nginx.ingress.kubernetes.io/ssl-verify: "false"
      host:
        name: admin-ceph.csepulveda.net
        path: /
        pathType: Prefix
      tls:
        - hosts:
          - admin-ceph.csepulveda.net
          secretName: admin-ceph-tls
      ingressClassName: nginx
  
  # Configuración del CephCluster  
  cephClusterSpec:
    # Placement global para todos los daemons de Ceph
    placement:
      all:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - node01
                - mode02
                - node03
    # Configuración adicional para RGW
    cephConfig:
      client.rgw:
        rgw_dns_name: "s3.csepulveda.net"
    cephVersion:
      image: quay.io/ceph/ceph:v18.2.4
      allowUnsupported: false
    dataDirHostPath: /var/lib/rook
    skipUpgradeChecks: false
    continueUpgradeAfterChecksEvenIfNotHealthy: false
    waitTimeoutForHealthyOSDInMinutes: 10
    
    mon:
      count: 3
      allowMultiplePerNode: false  # Un monitor por nodo para mejor distribución
      volumeClaimTemplate:
        spec:
          storageClassName: iscsi
          resources:
            requests:
              storage: 5Gi
    
    mgr:
      count: 2
      allowMultiplePerNode: false
      modules:
        - name: rook
          enabled: true
    
    dashboard:
      enabled: true
      ssl: true
    
    network:
      connections:
        encryption:
          enabled: false
        compression:
          enabled: false
        requireMsgr2: false
      ipFamily: "IPv4"
      dualStack: false
    
    crashCollector:
      disable: false
    
    logCollector:
      enabled: true
      periodicity: daily
      maxLogSize: 500M
    
    cleanupPolicy:
      confirmation: ""
      sanitizeDisks:
        method: quick
        dataSource: zero
        iteration: 1
      allowUninstallWithVolumes: false
    
    annotations: {}
    
    labels:
      all:
        rook.io/cluster: rook-ceph
      mgr:
        rook.io/cluster: rook-ceph
    
    resources:
      mgr:
        limits:
          memory: "1Gi"
        requests:
          cpu: "100m"
          memory: "512Mi"
      mon:
        limits:
          memory: "2Gi"
        requests:
          cpu: "100m"
          memory: "512Mi"
      osd:
        limits:
          memory: "4Gi"
        requests:
          cpu: "100m"
          memory: "2Gi"
      prepareosd:
        limits:
          memory: "400Mi"
        requests:
          cpu: "100m"
          memory: "50Mi"
      mgr-sidecar:
        limits:
          memory: "100Mi"
        requests:
          cpu: "50m"
          memory: "40Mi"
      crashcollector:
        limits:
          memory: "60Mi"
        requests:
          cpu: "10m"
          memory: "30Mi"
      logcollector:
        limits:
          memory: "1Gi"
        requests:
          cpu: "50m"
          memory: "100Mi"
      cleanup:
        limits:
          memory: "1Gi"
        requests:
          cpu: "100m"
          memory: "100Mi"
    
    removeOSDsIfOutAndSafeToRemove: false
    
    priorityClassNames:
      mon: system-node-critical
      osd: system-node-critical
      mgr: system-cluster-critical
    
    storage:
      # Configuración estricta para usar SOLO los discos iSCSI
      useAllNodes: false
      useAllDevices: false
      # Especificar nodos permitidos
      nodes:
      - name: node01
      - name: mode02
      - name: node03
      storageClassDeviceSets:
      - name: set1
        count: 3
        portable: false  # No portable para mantener OSDs en nodos específicos
        tuneDeviceClass: true
        tuneFastDeviceClass: false
        encrypted: false
        placement: {}
        preparePlacement: {}
        resources:
          limits:
            memory: "4Gi"
          requests:
            cpu: "100m"
            memory: "2Gi"
        volumeClaimTemplates:
        - metadata:
            name: data
          spec:
            resources:
              requests:
                storage: 20Gi
            storageClassName: iscsi
            volumeMode: Block
            accessModes:
              - ReadWriteOnce
      onlyApplyOSDPlacement: true  # Aplicar estrictamente el placement de OSDs
    
    disruptionManagement:
      managePodBudgets: true
      osdMaintenanceTimeout: 30
      pgHealthCheckTimeout: 0
    
    healthCheck:
      daemonHealth:
        mon:
          disabled: false
          interval: 45s
        osd:
          disabled: false
          interval: 60s
        status:
          disabled: false
          interval: 60s
      livenessProbe:
        mon:
          disabled: false
        mgr:
          disabled: false
        osd:
          disabled: false
  
  # Configuración del CephObjectStore
  cephObjectStores:
  - name: ceph-objectstore
    ingress:
      enabled: true
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt
        nginx.ingress.kubernetes.io/proxy-body-size: "0"
        nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
        nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
      host:
        name: s3.csepulveda.net
        path: /
      tls:
        - hosts:
          - s3.csepulveda.net
          secretName: s3-ceph-tls
      ingressClassName: nginx
    spec:
      allowUsersInNamespaces:
        - monitoring
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
          requireSafeReplicaSize: true
        parameters:
          compression_mode: none
      dataPool:
        failureDomain: host
        replicated:
          size: 3
          requireSafeReplicaSize: true
        parameters:
          compression_mode: none
      preservePoolsOnDelete: false
      gateway:
        port: 80
        instances: 2
        priorityClassName: system-cluster-critical
        hostNetwork: false
        placement: {}  # Sin restricciones adicionales
        resources:
          limits:
            memory: "2Gi"
          requests:
            cpu: "100m"
            memory: "512Mi"
      healthCheck: {}
    storageClass:
      enabled: true
      name: rook-ceph-bucket
      reclaimPolicy: Delete
      parameters:
        objectStoreName: ceph-objectstore
        objectStoreNamespace: rook-ceph
