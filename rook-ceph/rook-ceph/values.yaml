operator:
  crds:
    enabled: true
  
  monitoring:
    enabled: false
  
  csi:
    rookUseCsiOperator: false
    enableRbdDriver: false
    enableCephfsDriver: false
    enableNFSDriver: false
    enableCephfsSnapshotter: false
    enableRBDSnapshotter: false
    enableVolumeReplication: false
  
  enableDiscoveryDaemon: false
  resources:
    limits:
      memory: 256Mi
    requests:
      cpu: 50m
      memory: 64Mi

cluster:
  operatorNamespace: rook-ceph

  toolbox:
    enabled: true
  
  ingress:
    dashboard:
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt
        nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
        nginx.ingress.kubernetes.io/ssl-verify: "false"
      host:
        name: admin-ceph.csepulveda.net
        path: /
        pathType: Prefix
      tls:
        - hosts:
          - admin-ceph.csepulveda.net
          secretName: admin-ceph-tls
      ingressClassName: nginx
  
  cephClusterSpec:
    placement:
      all:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - node01
                - mode02
                - node03
    
    cephConfig:
      global:
        osd_pool_default_pg_num: "16"
        osd_pool_default_pgp_num: "16"
        osd_pool_default_pg_autoscale_mode: "off"
      client.rgw:
        rgw_dns_name: "s3.csepulveda.net"
      osd:
        osd_pool_default_pg_num: "8"
        osd_pool_default_pgp_num: "8"
    
    resources:
      mgr:
        limits:
          memory: "2Gi"
        requests:
          cpu: "50m"
          memory: "256Mi"
      mon:
        limits:
          memory: "1Gi"
        requests:
          cpu: "50m"
          memory: "256Mi"
      osd:
        limits:
          memory: "2Gi"
        requests:
          cpu: "50m"
          memory: "512Mi"
      prepareosd:
        limits:
          memory: "200Mi"
        requests:
          cpu: "25m"
          memory: "50Mi"
      mgr-sidecar:
        limits:
          memory: "50Mi"
        requests:
          cpu: "25m"
          memory: "25Mi"
      crashcollector:
        limits:
          memory: "30Mi"
        requests:
          cpu: "5m"
          memory: "15Mi"
      logcollector:
        limits:
          memory: "200Mi"
        requests:
          cpu: "25m"
          memory: "50Mi"
      cleanup:
        limits:
          memory: "200Mi"
        requests:
          cpu: "50m"
          memory: "50Mi"
    
    mon:
      count: 3
      allowMultiplePerNode: false
      volumeClaimTemplate:
        spec:
          storageClassName: iscsi
          resources:
            requests:
              storage: 5Gi
    
    mgr:
      count: 2
      allowMultiplePerNode: false
      modules:
        - name: rook
          enabled: true
    
    dashboard:
      enabled: true
      ssl: true
    
    network:
      connections:
        encryption:
          enabled: false
        compression:
          enabled: false
        requireMsgr2: false
      ipFamily: "IPv4"
      dualStack: false
    
    crashCollector:
      disable: false
    
    logCollector:
      enabled: true
      periodicity: daily
      maxLogSize: 500M
    
    cleanupPolicy:
      confirmation: ""
      sanitizeDisks:
        method: quick
        dataSource: zero
        iteration: 1
      allowUninstallWithVolumes: false
    
    labels:
      all:
        rook.io/cluster: rook-ceph
      mgr:
        rook.io/cluster: rook-ceph
    
    removeOSDsIfOutAndSafeToRemove: false
    
    priorityClassNames:
      mon: system-node-critical
      osd: system-node-critical
      mgr: system-cluster-critical
    
    storage:
      useAllNodes: false
      useAllDevices: false
      storageClassDeviceSets:
      - name: set1
        count: 3
        portable: true 
        tuneDeviceClass: false 
        tuneFastDeviceClass: false
        encrypted: false
        placement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - node01
                  - mode02
                  - node03
        preparePlacement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - node01
                  - mode02
                  - node03
        resources:
          limits:
            memory: "1Gi"
          requests:
            cpu: "50m"
            memory: "512Mi"
        volumeClaimTemplates:
        - metadata:
            name: data
          spec:
            resources:
              requests:
                storage: 20Gi
            storageClassName: iscsi
            volumeMode: Block
            accessModes:
              - ReadWriteOnce
      onlyApplyOSDPlacement: true 
    
    disruptionManagement:
      managePodBudgets: true
      osdMaintenanceTimeout: 30
      pgHealthCheckTimeout: 0
    
  cephObjectStores:
  - name: ceph-objectstore
    ingress:
      enabled: true
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt
        nginx.ingress.kubernetes.io/proxy-body-size: "0"
        nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
        nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
      host:
        name: s3.csepulveda.net
        path: /
      tls:
        - hosts:
          - s3.csepulveda.net
          secretName: s3-ceph-tls
      ingressClassName: nginx
    spec:
      allowUsersInNamespaces:
        - monitoring
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
          requireSafeReplicaSize: true
        parameters:
          compression_mode: none
      dataPool:
        failureDomain: host
        replicated:
          size: 3
          requireSafeReplicaSize: true
        parameters:
          compression_mode: none
      preservePoolsOnDelete: false
      gateway:
        port: 80
        instances: 2
        priorityClassName: system-cluster-critical
        hostNetwork: false
        placement: {}
        resources:
          limits:
            memory: "512Mi"
          requests:
            cpu: "50m"
            memory: "256Mi"
      healthCheck: {}
    storageClass:
      enabled: true
      name: rook-ceph-bucket
      reclaimPolicy: Delete
      parameters:
        objectStoreName: ceph-objectstore
        objectStoreNamespace: rook-ceph
